\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{doi}

\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\title{Statement of Purpose}
\author{Khalid Hossain \\
        Postdoctoral Appointee, Leadership Computing Facility \\
        Argonne National Laboratory}
\date{}
\begin{document}

\maketitle

I have started as a postdoctoral appointee in the ALCF division at Argonne in 
the beginning of 2023. My primary responsibilities include developing and 
doing performance 
analysis of scientific applications running on large distributed computational
platforms. During the course of my graduate studies I gained experience in 
doing HPC calculations in Summit at Oak Ridge. Although mostly in user 
capacity, I believe my experience will allow me to understand and learn the 
materials presented at ATPESC. The knowledge I will gain from the training will
put me in a strong position to continue computational research and develop a
career as a computational scientist.

\section*{Previous Experiences}

As part of graduate studies I have simulated quantum turbulence -- generation and
dynamical evolution of turbulent states in superfluid systems. The goal was to
develop experimentally relevant methods to reliably generate turbulent states
and explore decay mechanisms for these quantum states. In ordinary fluids the
decay of a turbulent state is attributed to the presence of viscosity which is
absent in its known form for superfluid systems. We have validated different 
processes involving interactions between quantized vortices which rise to an
effective viscosity facilitating the decay~\cite{Hossain:2022}.

Simulating experimentally relevant volumes require leadership class computing
facilities because of the size of associated states. As an extension of the 
physics problem discussed above we have done work on Summit, hosted by Oak Ridge
national laboratory. There we have ran successful simulation of a fermionic
superfluid system with $72^3$ lattice, where we diagonalized a $746,496^2$ 
Hamiltonian matrix to get the minimum energy turbulent state and then performed
subsequent time evolution using Adam-Bashforth-Moulton (ABM) predictor 
corrector method. The diagonalization calculation was distributed over $512$ 
nodes with $3072$ processors for $8$ self-consistent iterative loops. This is 
one of largest runs for such systems. We
also attempted a $96^3$ diagonalization, where the matrix size is $1,769,472^2$,
requiring $3072 - 4000$ nodes with $18432 - 24000$ processors for $2$ 
self-consistent loops. This was the first time we had a chance to explore the 
behavior of our code at such a large scale. During $96^3$ diagonalization we 
saw a degradation in performance, and could not obtain a converged initial state
for time evolution. For $72^3$, we had almost ideal scaling behavior and 
expected performance. We are proceeding with $72^3$ analysis, as it is sufficient 
to obtain physics results, $96^3$ would be an improvement in statistics.

This work has been done as part of an ALCC project~\cite{Alcc:2021}. We have 
used a custom time-dependent density functional theory (TD-DFT) based on 
superfluid local density approximation (SLDA) functional. The primary code is
written using CUDA for GPU processing, MPI for parallelization and ELPA linear
algebra libraries for diagonalization. This code has been released publicly as
W-SLDA toolkit~\cite{Bulgac:2014, W-slda:2023}.

My primary responsibilities were to prepare seed initial states for the 
diagonalization, job submission and scheduling scripts via batch jobs 
(using \verb|BSUB| and \verb|jsrun| commands),
ensure successful completion of simulations and perform data analysis by 
developing effective theories and models for turbulent dynamics in superfluids.

\section*{Research Plan}

In my current position, my research plans can be divided in roughly three 
assignments. These involves performing Deep Learning (DL) research using 
computational resources in an optimal fashion. To that end, one of my short 
term research plan is to complete a performance analysis for different available
frameworks (PyTorch, Tensorflow) used in different applications (convolutional
networks, graph networks etc.) with varying degree of precision (fp32, tf32, 
bf16 etc.). Another major aspect of this analysis is to find area
of improvements in cases of single-node, distributed and very large distributed
network performance. The success of this project depends on deeper 
understanding of distributed programming using MPI or OpenMP. The ATPESC 
workshop provides a solid foundation in that direction.

One of my long term projects focuses on DL applications in nuclear physics, more
specifically calculating ground state and static properties of nucleus using
Variational Monte Carlo (VMC) techniques. The simplified idea of VMC is to 
begin a calculation sequence with an estimate of the ground state energy, and
then minimize that to reach to a minimum energy state within acceptable 
accuracy. To make an initial estimate of the energy of the collection of 
nucleons we need to use an ansatz for the many-body wavefunction -- this is a
function of a few physical parameters which describes the distribution and 
density of particles. We will be using neural networks to make these ansatz
and use them to facilitate the calculation of different physical properties. 
The current state of the art solves the problem for nuclei up to $A=6$, where 
$A$ is the total number of neutrons and protons in a nucleus
~\cite{Adams:2021, Gnech:2022}. In these systems the typical number of trainable
network parameters are on the order of $\sim 10^4$, and our goal is to extend
the calculation to bigger nuclei, up to $A=20$. For such large systems it is
imperative that we take the advantage of distributed computing. We will be 
performing these calculations in leadership class facilities like Polaris and
Aurora, hosted by Argonne national laboratory. I will be actively contributing 
in validating and developing code to get physics results along with the
development of a benchmarking metric to compare performance across different
hardware setups.

I will be actively working on another long-term project where the goal is to 
bring in interesting quantum simulations for the supercomputer Aurora. In the
initial step this could be an extension of my work related to quantum 
turbulence (citation). The work may involve direct porting of the WSLDA (cite)
application to Aurora by converting it from CUDA to DPC++ (compatible with 
Intel GPUs) and exploring possible implementation with python frameworks like
PyTorch, TensorFlow and Jax. Along the same line of developing quantum 
applications, I also plan to explore many-body entanglement using random matrix
theory with potential application of DL methods.


\bibliography{reference}


\end{document}

